DAY-1:

relu - The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn
faster and perform better.
Loss function —This measures how accurate the model is during training. You want to minimize this function to "steer"
the model in the right direction.
EPOCH=number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through
the entire training dataset
The expand_dims() function is used to expand the shape of an array. Insert a new axis that will appear at the axis
position in the expanded array shape.

------------------------------------------------------------------------------------------------------------------------

DAY-2

$ Basic Text Classification
  - studies the subjective information in an expression, that is, the opinions, appraisals, emotions, or attitudes towards a topic, person or entity. Expressions can be classified as positive, negative, or neutral.
  -The OS module in Python provides functions for interacting with the operating system.
  -The shutil in Python is a module that offers several functions to deal with operations on files and their collections
  -use the Large Movie Review Dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. 
  -.cache() keeps data in memory after it's loaded off disk.
  -.prefetch() overlaps data preprocessing and model execution while training.
  -A legend is an area describing the elements of the graph.
  -Adam - is an adaptive learning rate optimization algorithm that's been designed specifically for training

$ FastAI basics 
  - History of Machine Learning 


------------------------------------------------------------------------------------------------------------------------

DAY-3

$Text classification with TensorFlow Hub: Movie reviews
  -binary—or two-class—classification
  -It uses the IMDB dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. 
  -tf.keras, a high-level API to build and train models in TensorFlow
  - tensorflow_hub, a library for loading trained models from TFHub in a single line of code
  -We use a pre-trained text embedding model from TensorFlow Hub called google/nnlm-en-dim50/2.
  -input_shape=[] ---> It's just python notation for creating a tuple that contains only one element.
  -binary_crossentropy is used cause it is better for dealing with probabilities—it measures the "distance" between probability distributions, 
                (In our case, between the ground-truth distribution and the predictions.)
  -verbose 0, 1 or 2 you just say how do you want to 'see' the training progress for each epoch. verbose=0 will show you nothing (silent)
  
$ Deep Learning for Coders
  - importance of labels in a dataset
  - feedback loops
  - untar_data is used to downnload and decompress dataset if it hasn't been 
  

---------------------------------------------------------------------------------------------------------------------------------
DAY-4 

$REGRESSION
  -In a regression problem, the aim is to predict the output of a continuous value, like a price or a probability. 
  -This notebook uses the classic Auto MPG Dataset and builds a model to predict the fuel efficiency of late-1970s and early 1980s automobiles
  -Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.
  -np.set_printoptions(precision=3) ----> Used to mention the precision we want
  - Pandas is a useful library in data analysis. It can be used to perform data manipulation and analysis.
  -na_values: This is used to create a string that considers pandas as NaN (Not a Number).
  -skipinitialspace  - It controls how the space following the delimiter will be interpreted. If True , the initial whitespaces will be removed. It defaults to False.
  -If there is at least one missing value in that column, the result is True. df. isna(). sum() returns the number of missing values in each column.
  -Pandas sample() is used to generate a sample random row or column from the function caller data frame
  -random_state: int value or numpy.random.RandomState, optional. if set to a particular integer, will return same rows as sample in every iteration.
  -A pairplot plot a pairwise relationships in a dataset.
  -The NumPy linspace function (sometimes called np. linspace) is a tool in Python for creating numeric sequences
  

---------------------------------------------------------------------------------------------------------------------------------

DAY-5

  $Tensors
    -Tensors are multi-dimensional arrays with a uniform type (called a dtype).
    -Tensors are like np.array
    -Tensors are immutable
    -Rank in Tensors: https://static.packt-cdn.com/products/9781787125933/graphics/B07030_14_01.jpg
    -You can convert a tensor to a NumPy array either using np.array or the tensor.numpy method:
    -matmul() function returns the matrix product of two arrays
    -The softmax function is simply a generalisation of the logistic function, which simply squashes values into a given range.
    -Shape : https://www.tensorflow.org/guide/images/tensor/4-axis_block.png
    -print(rank_3_tensor[:, :, 4]) ------>>> https://www.tensorflow.org/guide/images/tensor/index1.png
    -tf.reshape -----> Changes dimension of array (Eg (1,3)---->> (3.1))
    -A tensor with variable numbers of elements along some axis is called "ragged". Use tf.ragged.RaggedTensor for ragged data.
    -https://www.tensorflow.org/guide/images/tensor/ragged.png
    -tf.string is a dtype, which is to say you can represent data as strings (variable-length byte arrays) in tensors
    -Sometimes, your data is sparse, like a very wide embedding space. TensorFlow supports tf.sparse.SparseTensor and related operations to store sparse data efficiently.
    
  $Overfit and underfit
    -It's often possible to achieve high accuracy on the training set, what we really want is to develop models that generalize well to a testing set (or data they haven't seen before).
    -The opposite of overfitting is underfitting. Underfitting occurs when there is still room for improvement on the train data.
    -If you train for too long though, the model will start to overfit and learn patterns from the training data that don't generalize to the test data. 
    -To prevent overfitting, the best solution is to use more complete training data.
    -A model trained on more complete data will naturally generalize better. 
    -When that is no longer possible, the next best solution is to use techniques like regularization. 
    -The Pathlib module in Python deals with path related tasks, such as constructing new paths from names of files and from other paths, checking for various properties of paths and creating files and folders at specific paths.
    -tempfile — Generate temporary files and directories 
    -DATASET USED : Higgs dataset
    -The tf.data.experimental.CsvDataset class can be used to read csv records directly from a gzip file with no intermediate decompression step.
    -Dataset.cache method to ensure that the loader doesn't need to re-read the data from the file on each epoch:
    -number of learnable parameters in a model is often referred to as the model's "capacity".
    - model with more parameters will have more "memorization capacity"
    -Always keep this in mind: deep learning models tend to be good at fitting to the training data, but the real challenge is generalization, not fitting.
    -To find an appropriate model size, it's best to start with relatively few layers and parameters, then begin increasing the size of the layers or adding new layers until you see diminishing returns on the validation loss.
    -Many models train better if you gradually reduce the learning rate during training. Use optimizers.schedules to reduce the learning rate over time:
    -Syntax for InverseTimeDecay :

     tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None)
            * staircase	== Whether to apply decay in a discrete staircase, as opposed to continuous, fashion.
    
    -To reduce the logging noise use the tfdocs.EpochDots which simply prints a . for each epoch, and a full set of metrics every 100 epochs.
    -include callbacks.EarlyStopping to avoid long and unnecessary training times. Note that this callback is set to monitor the val_binary_crossentropy, not the val_loss
    

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

DAY 6 

$FastAI 

    -https://course.fast.ai/videos/?lesson=2
    -Learnt about fitting.
    -How our model cheats.
    -Classification vs regression
    -Validation Data 
    -How to choose Training Data set


--------------------------------------------------------------------------------------------------------------------------------------------------------

DAY 7

    -If you start overfitting training loss will go down and validation loss will go up
    -Transfer Learning - focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. 
    -Fine Tuning-. Fine-tuning deep learning involves using weights of a previous deep learning algorithm for programming another similar deep learning process.
    -Why Transfer Learning 
    -Vision techniques for sound
    -Detecting Virus using CNN
    -How to find predefined models
    -State of deep learning
    -Recommendation vs predictions
    -P value and how it shouldn't be ideally used
    -Null hypothesis significance


------------------------------------------------------------------------------------------------------------------------------------------------------------

DAY 8

$ Load Images : https://www.tensorflow.org/tutorials/load_data/images
  -Python Imaging Library is a free and open-source additional library for the Python programming language that adds support for opening, manipulating, and saving many different image file formats.
  -glob module is used to retrieve files/pathnames matching a specified pattern
  -The seed() method is used to initialize the random number generator.
  -astype() function comes very handy when we want to case a particular column data type to another data type.
  -The RGB channel values are in the [0, 255] range. This is not ideal for a neural network; in general you should seek to make your input values small. Here, we will standardize values to be in the [0, 1] by using a Rescaling layer.
  -Python next() function is used to fetch next item from the collection.
  -Python iter() function returns an iterator for the given object.
  -If you would like to scale pixel values to [-1,1] you can instead write Rescaling(1./127.5, offset=-1)
  -.cache() keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.
  -.prefetch() overlaps data preprocessing and model execution while training.
  -buffer_size: A tf. int64 scalar tf. Tensor, representing the maximum number elements that will be buffered when prefetching
  -Keras Conv2D is a 2D Convolution Layer, this layer creates a convolution kernel that is wind with layers input which helps produce a tensor of outputs.
  -Maximum pooling, or max pooling, is a pooling operation that calculates the maximum, or largest, value in each patch of each feature map
  -flatten() function we can flatten a matrix to one dimension in python
  -The shuffle() method takes a sequence, like a list, and reorganize the order of the items. Note: This method changes the original list, it does not return a new list.
  

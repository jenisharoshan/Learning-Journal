DAY-1:

relu - The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn
faster and perform better.
Loss function —This measures how accurate the model is during training. You want to minimize this function to "steer"
the model in the right direction.
EPOCH=number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through
the entire training dataset
The expand_dims() function is used to expand the shape of an array. Insert a new axis that will appear at the axis
position in the expanded array shape.

------------------------------------------------------------------------------------------------------------------------

DAY-2

$ Basic Text Classification
  - studies the subjective information in an expression, that is, the opinions, appraisals, emotions, or attitudes towards a topic, person or entity. Expressions can be classified as positive, negative, or neutral.
  -The OS module in Python provides functions for interacting with the operating system.
  -The shutil in Python is a module that offers several functions to deal with operations on files and their collections
  -use the Large Movie Review Dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. 
  -.cache() keeps data in memory after it's loaded off disk.
  -.prefetch() overlaps data preprocessing and model execution while training.
  -A legend is an area describing the elements of the graph.
  -Adam - is an adaptive learning rate optimization algorithm that's been designed specifically for training

$ FastAI basics 
  - History of Machine Learning 


------------------------------------------------------------------------------------------------------------------------

DAY-3

$Text classification with TensorFlow Hub: Movie reviews
  -binary—or two-class—classification
  -It uses the IMDB dataset that contains the text of 50,000 movie reviews from the Internet Movie Database. 
  -tf.keras, a high-level API to build and train models in TensorFlow
  - tensorflow_hub, a library for loading trained models from TFHub in a single line of code
  -We use a pre-trained text embedding model from TensorFlow Hub called google/nnlm-en-dim50/2.
  -input_shape=[] ---> It's just python notation for creating a tuple that contains only one element.
  -binary_crossentropy is used cause it is better for dealing with probabilities—it measures the "distance" between probability distributions, 
                (In our case, between the ground-truth distribution and the predictions.)
  -verbose 0, 1 or 2 you just say how do you want to 'see' the training progress for each epoch. verbose=0 will show you nothing (silent)
  
$ Deep Learning for Coders
  - importance of labels in a dataset
  - feedback loops
  - untar_data is used to downnload and decompress dataset if it hasn't been 
  

---------------------------------------------------------------------------------------------------------------------------------
DAY-4 

$REGRESSION
  -In a regression problem, the aim is to predict the output of a continuous value, like a price or a probability. 
  -This notebook uses the classic Auto MPG Dataset and builds a model to predict the fuel efficiency of late-1970s and early 1980s automobiles
  -Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.
  -np.set_printoptions(precision=3) ----> Used to mention the precision we want
  - Pandas is a useful library in data analysis. It can be used to perform data manipulation and analysis.
  -na_values: This is used to create a string that considers pandas as NaN (Not a Number).
  -skipinitialspace  - It controls how the space following the delimiter will be interpreted. If True , the initial whitespaces will be removed. It defaults to False.
  -If there is at least one missing value in that column, the result is True. df. isna(). sum() returns the number of missing values in each column.
  -Pandas sample() is used to generate a sample random row or column from the function caller data frame
  -random_state: int value or numpy.random.RandomState, optional. if set to a particular integer, will return same rows as sample in every iteration.
  -A pairplot plot a pairwise relationships in a dataset.
  -The NumPy linspace function (sometimes called np. linspace) is a tool in Python for creating numeric sequences
  

---------------------------------------------------------------------------------------------------------------------------------

DAY-5

  $Tensors
    -Tensors are multi-dimensional arrays with a uniform type (called a dtype).
    -Tensors are like np.array
    -Tensors are immutable
    -Rank in Tensors: https://static.packt-cdn.com/products/9781787125933/graphics/B07030_14_01.jpg
    -You can convert a tensor to a NumPy array either using np.array or the tensor.numpy method:
    -matmul() function returns the matrix product of two arrays
    -The softmax function is simply a generalisation of the logistic function, which simply squashes values into a given range.
    -Shape : https://www.tensorflow.org/guide/images/tensor/4-axis_block.png
    -print(rank_3_tensor[:, :, 4]) ------>>> https://www.tensorflow.org/guide/images/tensor/index1.png
    -tf.reshape -----> Changes dimension of array (Eg (1,3)---->> (3.1))
    -A tensor with variable numbers of elements along some axis is called "ragged". Use tf.ragged.RaggedTensor for ragged data.
    -https://www.tensorflow.org/guide/images/tensor/ragged.png
    -tf.string is a dtype, which is to say you can represent data as strings (variable-length byte arrays) in tensors
    -Sometimes, your data is sparse, like a very wide embedding space. TensorFlow supports tf.sparse.SparseTensor and related operations to store sparse data efficiently.
    
  $Overfit and underfit
    -It's often possible to achieve high accuracy on the training set, what we really want is to develop models that generalize well to a testing set (or data they haven't seen before).
    -The opposite of overfitting is underfitting. Underfitting occurs when there is still room for improvement on the train data.
    -If you train for too long though, the model will start to overfit and learn patterns from the training data that don't generalize to the test data. 
    -To prevent overfitting, the best solution is to use more complete training data.
    -A model trained on more complete data will naturally generalize better. 
    -When that is no longer possible, the next best solution is to use techniques like regularization. 
    -The Pathlib module in Python deals with path related tasks, such as constructing new paths from names of files and from other paths, checking for various properties of paths and creating files and folders at specific paths.
    -tempfile — Generate temporary files and directories 
    -DATASET USED : Higgs dataset
    -The tf.data.experimental.CsvDataset class can be used to read csv records directly from a gzip file with no intermediate decompression step.
    -Dataset.cache method to ensure that the loader doesn't need to re-read the data from the file on each epoch:
    -number of learnable parameters in a model is often referred to as the model's "capacity".
    - model with more parameters will have more "memorization capacity"
    -Always keep this in mind: deep learning models tend to be good at fitting to the training data, but the real challenge is generalization, not fitting.
    -To find an appropriate model size, it's best to start with relatively few layers and parameters, then begin increasing the size of the layers or adding new layers until you see diminishing returns on the validation loss.
    -Many models train better if you gradually reduce the learning rate during training. Use optimizers.schedules to reduce the learning rate over time:
    -Syntax for InverseTimeDecay :

     tf.keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None)
            * staircase	== Whether to apply decay in a discrete staircase, as opposed to continuous, fashion.
    
    -To reduce the logging noise use the tfdocs.EpochDots which simply prints a . for each epoch, and a full set of metrics every 100 epochs.
    -include callbacks.EarlyStopping to avoid long and unnecessary training times. Note that this callback is set to monitor the val_binary_crossentropy, not the val_loss
    

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

DAY 6 

$FastAI 

    -https://course.fast.ai/videos/?lesson=2
    -Learnt about fitting.
    -How our model cheats.
    -Classification vs regression
    -Validation Data 
    -How to choose Training Data set


--------------------------------------------------------------------------------------------------------------------------------------------------------

DAY 7

    -If you start overfitting training loss will go down and validation loss will go up
    -Transfer Learning - focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. 
    -Fine Tuning-. Fine-tuning deep learning involves using weights of a previous deep learning algorithm for programming another similar deep learning process.
    -Why Transfer Learning 
    -Vision techniques for sound
    -Detecting Virus using CNN
    -How to find predefined models
    -State of deep learning
    -Recommendation vs predictions
    -P value and how it shouldn't be ideally used
    -Null hypothesis significance


------------------------------------------------------------------------------------------------------------------------------------------------------------

DAY 8

$ Load Images : https://www.tensorflow.org/tutorials/load_data/images
  -Python Imaging Library is a free and open-source additional library for the Python programming language that adds support for opening, manipulating, and saving many different image file formats.
  -glob module is used to retrieve files/pathnames matching a specified pattern
  -The seed() method is used to initialize the random number generator.
  -astype() function comes very handy when we want to case a particular column data type to another data type.
  -The RGB channel values are in the [0, 255] range. This is not ideal for a neural network; in general you should seek to make your input values small. Here, we will standardize values to be in the [0, 1] by using a Rescaling layer.
  -Python next() function is used to fetch next item from the collection.
  -Python iter() function returns an iterator for the given object.
  -If you would like to scale pixel values to [-1,1] you can instead write Rescaling(1./127.5, offset=-1)
  -.cache() keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.
  -.prefetch() overlaps data preprocessing and model execution while training.
  -buffer_size: A tf. int64 scalar tf. Tensor, representing the maximum number elements that will be buffered when prefetching
  -Keras Conv2D is a 2D Convolution Layer, this layer creates a convolution kernel that is wind with layers input which helps produce a tensor of outputs.
  -Maximum pooling, or max pooling, is a pooling operation that calculates the maximum, or largest, value in each patch of each feature map
  -flatten() function we can flatten a matrix to one dimension in python
  -The shuffle() method takes a sequence, like a list, and reorganize the order of the items. Note: This method changes the original list, it does not return a new list.
  
-----------------------------------------------------------------------------------------------------------------------------------------------------------

DAY 9 

$CSV - Comma Separated Values
  -For any small CSV dataset the simplest way to train a TensorFlow model on it is to load it into memory
   as a pandas Dataframe or a NumPy array.
  -Abdalone DataSet = Predict the age of abalone from physical measurements
  - We use it cause:
    * The dataset is small.
    *All the input features are all limited-range floating point values.
  -If there is only a single input tensor, a keras.Sequential model is sufficient
  -It's good practice to normalize the inputs to your model.
  -The experimental.preprocessing layers provide a convenient way to build this normalization into your model.
  -The layer will precompute the mean and variance of each column, and use these to normalize the data.
  -Only use your training data to .adapt() preprocessing layers. Do not use your validation or test data.
  -In case of the different data types and ranges you can't simply stack the features into NumPy array and pass it to a 
    keras.Sequential model. Each column needs to be handled individually.
  -To build the preprocessing model, start by building a set of symbolic keras.Input objects,
     matching the names and data-types of the CSV columns.
  

-------------------------------------------------------------------------------------------------------------------------------------------------------------

DAY 10

  -If you need more control over the input data pipeline or need to use data that doesn't easily fit into memory: use tf.data.
  -Itertools is a module in python, it is used to iterate over data structures that can be stepped over using a for-loop.
  -The main difference between the cache and snapshot methods is that cache files can only be used by the TensorFlow process that created them, 
    but snapshot files can be read by other processes.
  -snapshot files are meant for temporary storage of a dataset while in use. This is not a format for long term storage.
   The file format is considered an internal detail, and not guaranteed between TensorFlow versions.
  -One place where tf.data can really simplify things is when dealing with collections of files.
  -When dealing with a bunch of files you can pass a glob-style file_pattern to the experimental.make_csv_dataset function.
   The order of the files is shuffled each iteration.
  -Use the num_parallel_reads argument to set how many files are read in parallel and interleaved together.
  -tf.io.decode_csv - a function for parsing lines of text into a list of CSV column tensors.
  -tf.data.experimental.CsvDataset - a lower level csv dataset constructor.
  
$ NumPy
  -NPZ is a file format by numpy that provides storage of array data using gzip compression.
  -RMSprop is a gradient based optimization technique used in training neural networks. ... This normalization balances the step size (momentum), 
    decreasing the step for large gradients to avoid exploding, and increasing the step for small gradients to avoid vanishing.

-------------------------------------------------------------------------------------------------------------------------------------------------

DAY 11 

$ PANDAS DATAFRAME
    -DataFrame can be created using a single list or a list of lists. 
    -DataFrame is a 2-dimensional labeled data structure with columns of potentially different types. 
    You can think of it like a spreadsheet or SQL table, or a dict of Series objects. It is generally the most commonly used pandas object.
     -pandas is a software library written for the Python programming language for data manipulation and analysis
    - Use tf.data.Dataset.from_tensor_slices to read the values from a pandas dataframe.
    -One of the advantages of using tf.data.Dataset is it allows you to write simple, highly efficient data pipelines. 
    -The easiest way to preserve the column structure of a pd.DataFrame when used with tf.data is to convert the pd.DataFrame to a dict,
       and slice that dictionary.
    -to_string() to print the entire DataFrame.
    -Load data using tf.data.Dataset
    -Create and train a model
    -Passing a dictionary as an input to a model is as easy as creating a matching dictionary of tf.keras.layers.Input layers,
     applying any pre-processing and stacking them up using the functional api. You can use this as an alternative to feature columns.
    
-----------------------------------------------------------------------------------------------------------------------------------------------

DAY 12 

  -The Python collection module is defined as a container that is used to store collections of data, for example - list, dict, set, and tuple, etc.
     It was introduced to improve the functionalities of the built-in collection containers. 
  - tf.data, it's a powerful collection of tools for building input pipelines.
  - use text_dataset_from_directory utility to create a labeled tf.data.Dataset which will load the data off disk and prepare
     it into a format suitable for training.
  - you can train a model by passing a tf.data.Dataset directly to model.fit. 
  -When using the validation_split and subset arguments, make sure to either specify a random seed, or to pass shuffle=False,
   so that the validation and training splits have no overlap.
  -Standardization refers to preprocessing the text, typically to remove punctuation or HTML elements to simplify the dataset.
  -Tokenization refers to splitting strings into tokens (for example, splitting a sentence into individual words by splitting on whitespace).
  -Vectorization refers to converting tokens into numbers so they can be fed into a neural network.
  -The default standardization converts text to lowercase and removes punctuation.
  -The default tokenizer splits on whitespace.
  -you will call adapt to fit the state of the preprocessing layer to the dataset. This will cause the model to build an index of strings to integers.
  -Note: it's important to only use your training data when calling adapt (using the test set would leak information).
  -  train model -


-----------------------------------------------------------------------------------------------------------------------------


DAT 13


  - Unicode is a standard encoding system that is used to represent character from almost all languages. 
  - Each character is encoded using a unique integer code point between 0 and 0x10FFFF
  -. A Unicode string is a sequence of zero or more code points.
  -The basic TensorFlow tf.string dtype allows you to build tensors of byte strings.
  - Unicode strings are utf-8 encoded by default.
  -A tf.string tensor can hold byte strings of varying lengths because the byte strings are treated as atomic units. The string length is not included in the tensor dimensions.
  -When using python to construct strings, the handling of unicode differs betweeen v2 and v3. In v2, unicode strings are indicated by the "u" prefix,
   as above. In v3, strings are unicode-encoded by default.
  -string scalar — where the sequence of code points is encoded using a known character encoding.
  -int32 vector — where each position contains a single code point.
  -The ord() function in Python accepts a string of length 1 as an argument and returns the unicode code point representation of the passed argument. 
   For example ord('B') returns 66 which is a unicode code point value of character 'B'.
  -tf.strings.unicode_decode: Converts an encoded string scalar to a vector of code points.
  -tf.strings.unicode_encode: Converts a vector of code points to an encoded string scalar.
  -tf.strings.unicode_transcode: Converts encoded string scalar to a different encoding.
  - length of the innermost dimension varies depending on the number of characters in each string:
  -You can use this tf.RaggedTensor directly, or convert it to a dense tf.Tensor with padding or a tf.SparseTensor using the methods
   tf.RaggedTensor.to_tensor and tf.RaggedTensor.to_sparse.
  -When encoding multiple strings with the same lengths, a tf.Tensor may be used as input:
  -If you have a tensor with multiple strings in padded or sparse format, then convert it to a tf.RaggedTensor before calling unicode_encode:
  -The tf.strings.length operation has a parameter unit, which indicates how lengths should be computed
  - unit defaults to "BYTE", but it can be set to other values, such as "UTF8_CHAR" or "UTF16_CHAR", 
  to determine the number of Unicode codepoints in each encoded string.
  -Similarly, the tf.strings.substr operation accepts the "unit" p
  
  arameter, and uses it to determine what kind of offsets the
   "pos" and "len" paremeters contain
  -The tf.strings.unicode_split operation splits unicode strings into substrings of individual characters:
  -Each Unicode code point belongs to a single collection of codepoints known as a script . A character's script is helpful in determining
   which language the character might be in. For example, knowing that 'Б' is in Cyrillic script
   indicates that modern text containing that character is likely from a Slavic language such as Russian or Ukrainian.
  -TensorFlow provides the tf.strings.unicode_script operation to determine which script a given codepoint uses. The script codes are int32 values
   corresponding to International Components for Unicode (ICU) UScriptCode values
   -Segmentation is the task of splitting text into word-like units. 


--------------------------------------------------------------------------------------------------------------------------------------------------------


DAY 14

-DataLoaders:DataLoader returns the batch as a list with the batch as the only entry.
-A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known
 from pyramidal cells in the cerebral cortex.
 Residual neural networks do this by utilizing skip connections, or shortcuts to jump over some layers.
-: “Null hypothesis significance testing (NHST) is a method of statistical inference by which an observation is tested 
against a hypothesis of no effect or no relationship.”
-bing image api
-DataBlock Api

-----------------------------------------------------------------------------------------------------------------------------------------------------------

DAY 15
   
-A tensor is a generalization of vectors and matrices to potentially higher dimensions. 
  Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes.
-They are the main objects that are passed around and manipluated throughout the program. 
-Each tensor represents a partialy defined computation that will eventually produce a value. TensorFlow programs work by building a graph of Tensor objects that details how tensors are related.
-Running different parts of the graph allow results to be generated.
-Each tensor has a data type and a shape.
-Data Types Include: float32, int32, string and others.
-Just like vectors and matrices tensors can have operations applied to them like addition, subtraction, dot product, cross product etc.